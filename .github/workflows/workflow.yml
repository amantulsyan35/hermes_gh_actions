name: Web Content Fetcher with Turso Database

on:
  schedule:
    - cron: "0 0 * * *" # Run daily at midnight
  workflow_dispatch: # Allow manual triggering

jobs:
  fetch-and-store-web-content:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 libsql-experimental

      # Run Web content fetcher script (don't need Turso token here)
    -  name: Run Web content fetcher script
        env:
        API_ENDPOINT: "https://open-source-content.xyz/v1/web"
        TURSO_URL: "libsql://context-amantulsyan35.aws-us-east-1.turso.io"
        TURSO_AUTH_TOKEN: ${{ secrets.TURSO_AUTH_TOKEN }}
        run: python scripts/fetch_web.py

      # # Migrate data to Turso database (needs Turso token)
      # - name: Migrate data to Turso
      #   env:
      #     SQLITE_DB: "content.sqlite"
      #     TURSO_URL: "libsql://context-amantulsyan35.aws-us-east-1.turso.io"
      #     TURSO_AUTH_TOKEN: ${{ secrets.TURSO_AUTH_TOKEN }}
      #   run: python scripts/sqlite_to_turso.py

      # Upload database and results as artifacts (for backup)
      - name: Upload database
        uses: actions/upload-artifact@v4
        with:
          name: database-files
          path: |
            content.sqlite
            data/processed_weblinks.json
          retention-days: 5
